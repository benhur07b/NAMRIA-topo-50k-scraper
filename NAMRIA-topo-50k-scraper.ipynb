{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAMRIA Topo Map (1:50,000) Scraper\n",
    "\n",
    "### About\n",
    "This Notebook is used for scraping/downloading the 1:50,000 topographic maps available at the NAMRIA [website](\"http://www.namria.gov.ph/download.php\") and serves as a basic exercise in scraping using Beautiful Soup and Python 3.\n",
    "\n",
    "### RUNNING THE TOOL\n",
    "\n",
    "_**REQUIREMENTS**_\n",
    "* Python 3.6.1\n",
    "* jupyter\n",
    "* Beautiful Soup\n",
    "\n",
    "They are also found in the **requirements.txt** file.\n",
    "\n",
    "You can install these requirements using **pip** (sudo **pip install -r requirements.txt** or **pip install -r requirements.txt**)\n",
    "\n",
    "_**PROCEDURE**_\n",
    "1. Add the proxy in PROXY (if any).\n",
    "2. Select the directory to save the scraped files to (SAVEDIR).\n",
    "3. Add a QUERY list to limit the download (i.e. [\"3343\"]; only files containing strings matching any of the elements in QUERY will be downloaded).\n",
    "4. Run All.\n",
    "\n",
    "### LICENSE\n",
    "_Copyright (C) 2017 Ben Hur S. Pintor (bhs.pintor@gmail.com)_ [[website](\"https://benhur07b.github.io\")]\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
    "You should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import shutil\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "BASE_URL = \"http://www.namria.gov.ph/\"\n",
    "TOPO_URL = \"http://www.namria.gov.ph/topo50Index.aspx/\"\n",
    "\n",
    "PROXY = \"\"  # insert proxy here\n",
    "\n",
    "PROXIES = {\"http\": PROXY}\n",
    "\n",
    "SAVEDIR = \"\"  # add directory to save scraped files here\n",
    "\n",
    "QUERY = [\"\"]  # add query items (strings to limit downloads) here, leave empty to download ALL data\n",
    "\n",
    "if SAVEDIR:\n",
    "    if not os.path.exists(SAVEDIR):\n",
    "        os.makedirs(SAVEDIR)\n",
    "        \n",
    "    os.chdir(SAVEDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_links(r, query):\n",
    "    \"\"\"Returns a list of links in a page provided for by a requests object r.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    r -- the requests object\n",
    "    \"\"\"\n",
    "    \n",
    "    links = list()\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    for link in soup.find_all('area'):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    if len(query) > 0:\n",
    "        return [l for l in links if any(q in l for q in query)]  # return only the links that match any of the query items\n",
    "                                                                 # for every l in links, checks if l contains any q in query\n",
    "    else:\n",
    "        return links\n",
    "\n",
    "    \n",
    "def save_to_file(url, proxies=None):\n",
    "    \"\"\"Saves the topo map in the url into an image.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- the url of the sensor measurement\n",
    "    proxies -- the proxy settings, if any (default = None)\n",
    "    \"\"\"\n",
    "    \n",
    "    if proxies:\n",
    "        r = requests.get(url, stream=True, proxies=proxies)\n",
    "    else:\n",
    "        r = requests.get(url, stream=True)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    for img in soup.find_all('img'):\n",
    "        img_src = img.get('src')\n",
    "        img_name = img_src.split(\"/\")[-1]\n",
    "        if proxies:\n",
    "            img_topo = requests.get(\"{}{}\".format(BASE_URL,img_src), stream=True, proxies=proxies)\n",
    "        else:\n",
    "            img_topo = requests.get(\"{}{}\".format(BASE_URL,img_src), stream=True)\n",
    "                \n",
    "        with open(img_name, 'wb') as outfile:\n",
    "            shutil.copyfileobj(img_topo.raw, outfile)\n",
    "\n",
    "        del img_topo\n",
    "        print(\"{} saved.\".format(img_name))\n",
    "\n",
    "    \n",
    "r = requests.get(TOPO_URL, proxies=PROXIES)\n",
    "links = get_links(r, QUERY)\n",
    "for link in links:\n",
    "    if not os.path.exists(link):\n",
    "        save_to_file(\"{}{}\".format(BASE_URL, link), PROXIES)\n",
    "    else:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
